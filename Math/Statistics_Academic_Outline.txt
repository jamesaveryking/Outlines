Statistics Academic Outline 6/28/17

Basic Definitions:
-StatisticS: the study of methods for collecting, organizing, and analyzing data
	-Descriptive Statistics: procedures used to organize and present data in a convenient and communicable form
	-Inferential Statistics: procedures employed to arrive at broader conclusions or inferences about populations on the basis of samples
-Population: the complete set of actual or potential elements about which inferences are made
-Sample: a subset of the population selected using some sampling method
	-Sampling Methods:
		-cluster sample: a population is divided into groups called clusters; some clusters are randomly selected, and every member in them is observed
		-stratified sample: the population is divided into strat, and a fixed number of elements of each stratum are selected for the sample
		-simple random sample: a sample selected so that each possible sample of the same size has an equal probability of being selected; used for most elementary inference
-Variable: an attribute of elements of a population or sample that can be measured (height, weight, etc.)
	-categorical -- variables that maintain a limited set of possible values -- (meals of the day{breakfast, lunch, dinner})
	-continuous -- variables that are not limited in what values they can have or have a large number of possible values
-Types of measurements:
	-univariate: one measurement per object
	-bivariate: two measures per object
-Data: values of variables that have been observed
	-Types of data:
		-qualitative: descriptive but not numeric
		-quantitative: have numeric values
		-discrete: take counting numbers as values usually representing data that can be counted (rating{0:10})
		-continuous: can take a range of numeric values, not just counting numbers (numbers with decimals, weights of a bag of rice)
	-Levels of measurement:
		-qualitative levels:
			-nominal: values are simply named without order (color of a bean)
			-ordinal: values have some natural order (grades in school)
		-quantitative levels:
			-interval: numeric data with no natural zero point (intervals are meaningful ratios are not (range of temperature {32F:100F}))
			-ratio: numeric data for which there is a true zero; both intervals and ratios are meaningful (weight, length, duration, most physical properties)
-Statistic: a numeric measurement computed from sample dta, used to deescribe the sample and to extimate the corresponding population parameter
-Parameter: a numeric measurement that describes a population; parameters are usually not computed, but are inferred from sample statistics

-Types of Descriptive Methods:
	-tabular: frequency distribution table -- gives all possible values of variables and their frequencies
		-frequency of a value, usually denoted with "f"
		-relative frequency of a value -- ratio of the frequency to the total number of values
		-cumulative frequency -- the number of observations less than or equal to a specified value -- usually denoted "cf"
	-graphical: barcharts, pie charts (use each percent as 3.6 degrees of chart)
		-examining graphs:
			-center, spread (using ranges and broadness/narrowness), shape (symmetric, left-skewed, right-skewed)
			-clusters and gaps -- observe groupings of data
			-outliers -- any observation that is surprisingly different
	-numerical
-Frequency Distributions: proveds the frequency (number of observations) or each value of a variable
	-Grouped Frequency Distributions: values of the variable are grouped into classes
	-Relative Frequency Distributions: each frequency is divided by the total number of observations to produce the proportion or percentage of the data set having that value
	-Cumulative Frequency Distribution: frequencies account all observations at a particular value or class and all those less
	
-Measurements of Central Tendency:
	-Mean: most commonly used measure of central tendency, usually meant by "average", sensitive to extreme values
		-population mean is calculated separately from sample mean with parallel formulas
		-population mean -- greek letter mu || sample mean -- uppercase X with bar over it
			mean = (1/(total number in population or sample))*(sum of all values in population or sample)
		-trimmed mean: computed discarding some number of the highest and lowest values; less sensitive than ordinary mean
		-weighted mean: computed with a weight multiplied by each value , making some values influence the mean more heavily than others
	-Median: value that divides the set so the same number of observations lie on each side of it; less sensitive to extreme values; for an odd number of values, it is the middle value; for an even number, it is the average of the middle two
	-Mode: observation that occurs with the greatest frequency

-Measures of Dispersion:
	-sum of squares (SS): the sum of squared deviations from the mean:
		-can be by population or sample
			SS: (sum of all values in a sample or population minuse the sample or population mean)**2 || ((sum of all values in a sample or population squared)-((sum of all values in sample or population squared)/(number of values in sample or population))
	-variance: the average of square differences between observations and their mean
		-can be by population or sample
			-variance of population = (1/total number in population)*((sigma on all values in population - population mean)**2)
			-variance of sample = (1/(total number in sample - 1)) * ((sigma on all values in sample - sample mean)**2)
		-can also be specified for grouped data:
			-population grouped variance = (1/total number in population)*(sigma on ((f sub i) * (value in population - population mean))**2)
			-sample grouped variance = (1/(total number in sample - 1))*(sigma on ((f sub i) * (value in sample - sample mean))**2)
	-standard deviation: the square root of the variance; unlike variance, it has the same units as the original data and is more commonly used
		-population SD: square root((1/total number in population)*(sigma on (all values in population - population mean)**2)
	-standard scores: also known as z-scores; teh standard score of a value is the directed number of standard deviations form the mean at which the value is found -- z = ((value - respective mean)/(respective deviation)
		-positive z score indicates a value greater than the mean -- negative z score indicates a value less than the mean -- z score of 0 indicates matching the mean
		-converting every value in a data set or distribution to a z score is called standardization
			-once a data set or distribution has been standardized, it has a new mean = 0, and a new standard deviation of 1

-Graphing Techniques:
	-Bar Graph: a graph that uses bars to indicate the frequency of occurrence of observations
		-histogram: a bar graph used with quantitative, continuous variables
	-Frequency Curve: a graph representing a frequency distribution in teh form of a continuous line that traces a histogram
		-cumulative frequency curve: a graph representing a frequency distribution in the form of a continuous line that traces a histogram
		-symmetric curve: the frequency curve is uncahnged if rotate around its center; median = mean
		-normal curve: bell-shaped curve; symmetric
		-skewed curve: deviates fomr symmetry; frequency curve is shifted with a loger tail to the left (mean < median) or to the right (mean > median)
Probability: a measure fo the likelihood of a random event; the long-term relative frequency wiht which an outcome or event occurs
	-formula: probability of occurences(A) = number of outcomes favoring event A / total number of outcomes
	-sample spaces: all possible simple outcomes of an experiment
	-relationships between events:
		-exhaustive: 2 or more events are said to be exhaustive if they represent all possible outcomes
			- P(A|B|C...) = 1
		-non-exhaustive: 2 or more events are said to be non-exhaustive if they do not represent all possible outcomes
		-mutually exclusive: events that cannot occur simultaneously -- P(A&B) = 0 and P(A||B) = P(A)+P(B)
		-non-mutually exclusive: events that can occur simultaneously -- P(A||B) = P(A)+P(B)-P(A&B)
		-independent: events whos probability is unaffected by occurrence or nonoccurrence  of each other 
			P(A||B) = P(A) or P(A||B) = P(B) or P(A&B) = P(A)*P(B)
Graphical Methods for Continuous Variables:
		-Dotplots: (parallel to line ruler)
			1) draw a horizontal line (x-axis) to indicate data range
			2) scale the line to accomodate entire range or note outliers if trimmed
			3) mark a dot for each oservation in the appropriate place above the scaled line
			4) if more than one observation has the same value, add dots on top (one above the other)
			-each dot indicates location of value at that point
			-observations are useful in viewing: data spread, shape of collective data, approximate center of distribution
		-Stemplots: (also stem-and-leaf plots, when turned on it's side it resembles a histogram)
			-shows every value but inconvenient on large data sets
			1) separate each observation into two parts -- left is stem, right is leaf
				-use reasonable number of stems
			2) draw a vertical line on the left side of the page to separate stems and leaves
			3) write outall possible stems in increasing order on the left of the line to ensure entire range of data is covered
			4) for each observation write the leaf to the right of the corresponding stem on the right side of the vertical line
			5) after leaves are created for all observations write all leaves with the same stem in increasing order
			-observations are useful when viewing: how variable is spread (rotate 90 degrees), shape of distribution of the variable, center of the variable
			-special type to compare two boxplots -- back-to-back stem plot
		-Histograms: (looks like a stem plot on its side, useful on large data sets -- not useful on small data sets, pattern within data group is lost but overall pattern remains visible)
			1) create groups of equal length
			2) draw the x and y axes 
			3) scale the x-axis to accomodate large data groups
			4) scale the y-axis to accomodate the range of frequencies used (or relative frequencies or percentages)
			5) draw bars of heights equal to corresponding frequences and add a label for each group 
				-draw bars without gaps in-between
			-most popular way to display data
		-Cumulative Frequency Charts: frequency of that group plus sum of all frequencies up to that observation or group (also called ogive chart)
			1) draw the x and y axes
			2) scale the x-axis to accomodate the range of all groups, mark the upper boundary of each group
			3) scale the y-axis {0:n} for a cumulative frequency ({0:1} for relative frequency)
			4) draw point at height equal to the cumulative frequency for that group above the upper bound
			-symmetric resembles S, right-skewed is above symmetric to the left, left-skewed is below symmetric to the right
		-Boxplots: also box-and-whisper plot -- graphical data summary based on measures of position
			-useful in identifying outliers and general shape of the distribution
			1) draw a horizontal number line
			2) scale the number line to cover the range of observations
			3) draw a regular box above the line from teh first ot the third quartiles
			3) draw a vertical line at the MEDIAN to divide the two components
			4) computer the "whisker" length = ((3/2)*IQR = WL
			5) Compute lower whisker = Q1 - WL, upper whisker = Q3 + WL
			6) make a lower and upper whisker by the smallest and largest values connected to Q1 and Q3 respectively
			7) plot any points larger or smaller than U or L above or below U or L
		-Multiple Frequency Polygrams: (Line Graphs)
			1) connect points to top of bars in a histogram
	-Describing a distribution:
		-when examining a graphical summary of a distribution of univariate data use the following three measures to describe the data:
			-center, spread, shape
			-additionally not clustering, gaps, and any outliers -- provide explanations if possible
Numerical Measures:
	-measures of central tendency (mean, median, mode)
	-measures of variation (standard deviation, variance, range, interquartile range, standard deviation (distance from mean -- 0 means all measurements uniform -- larger indicates larger spread))
	-measures of position (quartiles (1--25%,2--50%,3--75%), percentiles (divide set of values into 100 equal parts (see below)), standardized z-scores (distance between a number and the mean in terms of the standard deviation((measurement-mean)/standard deviation))
		-percentiles -- n measurements, kth percentile, formula on exact percentile {0:100} = ((n+1)*k)/100
	-effects of changing units on summary measures
		-addition: adds to mean, median, and quartiles, does not change range, standard deviaiton, or interquartile range
		-multiplication: multiplies all -- mean, median, range, standard deviation (absolute value only), quartiles, and interquartile range
Comparing Distributions of 2+ groups:
	-compare the centers of the distributions
	-compare the spread -- both inter and intra
	-clusters of measurements and gaps in measurements
	-outliers and any other unusual features
	-compare the shapes of the distributions
Exploring Bivariate Data:
	-bivariate data: collected on two different variables from each item in a study
	-two common summaries:
		-scatterplot -- graphical representation
			-used to describe nature, dgree, and direction of the relation between two variables
			1) draw the x and y axes
			2) scale the x axis to accomodate range of first variable
			3) scale y axis to accomodate range of other variable
			4) for each pair of measurements, mark the point on the graph where the x and y values intersect
			-here is what can be derived about relationship of two variables:
				-shape -- linear or nonlinear
				-direction -- shows whether y value increases or decreases as x increases or that it changes direction
				-strength of relationship -- linear regression, allows to determine correlation
					-close points -- strong
					-some scattered -- weak
					-no pattern -- no relationship
		-correlation coefficient -- numerical representation
Correlation Coefficient: numerical measure used to judge the linear relation between two variables -- does not work well with nonlinear
	-Pearson's correlation coefficient (usually just correlation coefficient) is a numeric measure of the degree and direction of the linear relaion between two quantitative variables
		-population represented by rho in greek alphabet || sample represented by "r"
			-range = {-1:1} always
		-indicates direction (positive (x increase, y increaases linearly) or negative(x increase, y decreases linearly))
		-indicates strength:
			-- 1 indicates perfect positive correlation
			-- -1 indicates perfect negative
			-- close to 1 or -1 -- strong
			-- close to 0 -- weak
			-- 0 -- no relationship
			-- not close to 0 indicates stronger relationship
	-Least Squares Regression Line: useful in predicting corresponding values of one variable for known values of the other variable
		-linear regression model or linear regression equation gives a straight line relationship between two variables
			y = (x*m) + b + e
			-y is the dependent variable or response variable
			-m is the slope
			-b is the y-intercept
			-x is the independent variable
			-e is the random error or residual
		-predicted valeu of y is denoted by "y hat"
			-computed using the regression line: y hat = a + bx
				-a is the y-intercept of regression line
				-b is the estimated slope of regression line
			-residual ^^^ e is computed by y-"y hat"
		-least square regression line minimizes teh error sum of squares of the residuals
			-line of best fit -- always passes through average of x and y as a point -- ("x bar, y bar")
		-coefficient of determination -- measures the percent of variation in y-values explained by the linear relation between x and y values
			-percent variation in y values attributable to percent variation in x values
			-denoted R**2
				-in linear regression is equal to the square of Pearson's coefficient
		-outliers and influential points 
			-outlier -- suprprisingly different from the rest of the data
			-influential observation -- strongly affects a statistic
		-residual plot: a plot of residuals versus the predicted values of y -- used to assess the fit of the model
			-any trends in residual indicate that linear model is not accurate
-Transforms to achieve linearity
	-nonlinear models
	-transformation of the relation -- example) y=(a*(X**b)) -- natural log can be taken of both sides
		-square root is the most commonly applied power transformation
		-other examples:
			-log transformation -- used to linearize the regression model when the relaionship between y adn x suggests a model with a consistently increasing slope
			-square root transformation -- used when the spread of the observation increases with the mean
			-reciprocal transformation -- used to minimize effects of large values of x
			-squre transformation -- used when the slope of the relation consistently decreases as the independent variable increases
			-power transformation -- used if the relationship between dependent and independent variables is modeled by y=(a*(X**b))
-Exploring Categorical Data: Frequency Tables
	-r categories of classification in criterion 1 and c categories of classification in criterion 2 -- known as r * c contigency table
	-joint frequency -- of two categories is the frequency with which two categories, one from each of the two classification criteria, occur
		-row and column totals yield marginal frequencies with which each category occurs
-Conditional Relative Frequencies and Association
	-conditional relative frequency -- relative frequency of one category given that the other category has occurred
		-determines if association exists between the two classification criteria
	-association -- measures degree of relation between categorical variables
		-if there is no assocation between two criteria the number of measurements in a given cell of the contingency table is equal to:
			((row total)*(column total))/total number of measurements
				-if value is greater than observed, there is a negative association
				-if value is less than observed, there is a positive association

-Overview of methods of data collection
	-terms:
		-population: the entire group of individuals or items that we are interested in
		-frame: list of all members of the population
		-sample: part of the population that is actually being examined
		-sample survey: process of collecting information out of a sample
		-censue: process of collecting information from all units in a population
	-experiments and observational studies:
		-experiment: a planned activity that results in measurements (data or observations) 
			-experimenter creates differences in the variable involved in the study and then observes the effects of such differences on the resulting measurements
		-observational study: activity in which the experimenter observes the relationships among variables rather than creating them
	-planning and conducting surveys:
		-biased sample method: result in values that are systematically different from the population values or systematically favor certain outcomes
			-judgemental sampling -- non-random approach, samples of convenience, volunteer samples
	-simple random sampling -- process of obtaining a sample from a population in which each member has an equal chance of being selected
		-sampling with replacement from a finite population -- drawing cards after returning them to deck
		-sampling without replacement from an infinite (or large compared to sample size) population -- selecting voters from a voter registry in a city
		-selected with a single blind method -- population does not know who is selected but surveyor knows algorithm of survey
	-other methods of random sampling:
		-systematic sampling-- first item is selected at random from teh first k items in a set and then every kth item is included in the sample
		-stratified random sampling -- population is divided into groups called strata (singular is stratum) and a simple random sample is selected from each stratum 
			-stratum are homogenous parts of population units
		-proportional sampling -- population is divided into strat and a simple random sample of size proportional to stratum size is selected from each stratum 
		-cluster sampling -- a population is divided into nonhomogenous groups called clusters and a simple ranodm sample is obtained from some clusters, but not necessarily all

-Bias in surveys
	-sampling error -- variation inherent in any survey -- outcome is never uniform in sampling due to dynamic nature of majority of populations
	-response bias -- caused by behavior of interveiwer or respondent -- personal bias violates objectivity (someone not wanting to admit to smoking in high school)
	-nonresponse bias -- no contact or answer refusal
	-undercoverage bias -- part of population is left out of selection
	-wording effect bias -- potential subjective wording could distort survey objectivity

-Planning and Conducting experiments
	-response (dependent) variable is the variable to be measured
	-explanatory (independent) variable explains difference in responses (usually known variable that changes over time to elicit a response's response)
	-experimental unit -- smallest unit of population to which treatment is applied
	-confounding variable -- variable whose effect on the response cannot be separated from the explanatory variable
	-factor -- variable whose effect on the response is of interest in the experiment
		-qualitative or quantitative (categorical or numeric in value)
		-levels -- values of factors in an experiment
		-treatments -- factor-level combinations used in the experiment
	-control group -- group of experimental units similar to all other experimental units except that it is not given any treatment
	-placebo group -- group given fake treatment involving medication
	-single and double blind:
		-blinding technique -- used in medical experiments to prevent such a bias
		-single-blind -- either surveyed OR surveyor do not know which treatment is given
		-double-blind -- surveyed AND surveyor do not know which treatment is given
	-randomization should be implemented in both treatment on groups of population and order of treatments 
	-blocking -- control effects of unknown factors
		-block -- group of homogenous experimental units
	-replication -- process of giving a certain treatment numerous times in an experiment or of applying it to a number of different experimental units and getting similar results
	-completely randomized design -- treatments are assigned to all experimental units or expermental units are assigned randomly to all treatments
	-randomized block design: grouping yields better results
		-randomized paired comparison design -- used when only two treatments are present
			-can be implemented in two ways:
				-matched pair design -- groups are assigned by tossing a coin
				-experimental units are used as their own blocks

-Anticipating Patterns
	-probability -- measure of likelihood of an event
		-percent -- special probability expressed as a decimal value with at least hundredths precision multiplied by 100 to have a number between 0 and 100
	-sample space -- set of all possible outcomes of an experiment (tossing coin = heads/tails, rolling dice = [1:6]
		-tree diagrams are useful in determining sample space of an experiement with layers of tree representing different applications of new modular sample spaces
	-probability(a) = number of outcomes that lead to "a" / total number of possible outcomes
	-basic probability rules and terms:
		-for any event the probability is always: 0<=P<=1
		-sum of all probabilities contained in a sample space is always one
		-impossible event -- probability is 0
		-sure event -- probability is 1
		-odds in favor of an event = probability of event / probability event does not occur
			-complement -- set of all possible outcomes in a sample space that does not lead to event -- (1-P(X))
		-disjoint or mutually exclusive events -- no outcome in common -- P(X AND Y) = 0
		-union of two events -- set of all possible outcomes that lead to at least one of the two events -- X OR Y -- P(X) + P(Y) - P(X AND Y)
		-intersection of two events -- set of all possible outcomes that lead to both events -- X AND Y -- P(X) * P(X|Y) = P(Y) * P(X|Y)
		-conditional event -- set of outcomes for one event if another has occurred -- X GIVEN Y -- P(X|Y) = P(X AND Y) / P(Y)
			-independent events -- occurrence of one event does not depend on the occurrence of the other -- P(X|Y) = P(X) AND P(Y|X) = P(Y) -- IFF P(X AND Y) = P(X) * P(Y)
			-dependent events -- occurrence of one event does depend on the occurence of another event
	-random variables:
		-variable -- any quantity whose value varies from subject to subject
		-probability experiment -- possible outcomes may be known but exact outcome is a random event and cannot be predicted with accuracy in advance
			-if outcome is numerical, outcome is a quantitative variable
			-random variable -- outcome of probability experiment, usually denoted with capital letters
				-two types:
					-discrete -- quantitative variable that takes a countable number of values 
					-continuous -- quantitative variable that can take all possible values in a given range (weight, rainfall, etc.)
		-probability distributions of random variables:
			-discrete probability distribution -- table list graph, or formula giving all possible values taken by a random variable
			-mean of a random discrete variable is known as expected value -- denoted with E(X) and greek letter mu 
				-computed by multiplying every value of the random variable by its respective probability and then adding over the sample space
				-mu = E(X) = sum(i=1:n) of ith x * P(ith x)
			-variance -- sum of the product of squared deviations of the values of the variable from the mean and the corresponding probabilities
				-standard deviation is the square root of variance
				-omega**2 = sum(i=1:n) of (ith x - mu)**2 * P(ith x)
	-combinations -- number of ways X can be selected out of Y items if the order of selection is NOT important
		-denoted {X}		X! / (Y!)*(X-Y)! -- remember 0! = 1! = 1
				 {Y}	 = 
	-binomial distributions: distribution of discrete random variables with "n" repeated trials, all trials are identical and independent, each trial has two possible outcomes generally known as a success or a failure
		X = number of successes in "n" trials
		P(x successes in n trials) = {n} = (p**x)*(1-p)**(n-x) 
									 {x}
			where P(...) is the success of a given trial
			the mean (mu) of a binomial random variable = E(x) = n*p
			the variance of a binomial random variable = (sigma**2) = n*p*(1-p)
		-always use when determining success or failure of outcomes and translate information given into a success or failure
		-fixed number of trials but varying number of successes
	-geometric distribution: occurs when repeated trials are repeated until first success is observed, all trials are identical and independent, each trial has success or failure outcomes
		P(x trials needed until the first success is observed) = (1-p)**(x-1)
			the mean (mu) = E(x) = (1-p)/p
			variance = sigma**2 = ((1-p)**2)/p
		-example: rewrites on a program until a successful output, number of bugs in debugging until successful program output
		-fixed number of successes but varying number of trials (like a while loop checking for a condition)
	-probability distributions of continuous random variables: graph giving all possible values taken by a random variable and the corresponding probabilities
		-total probability under curve = 1
		-area under curve is equal to the probability
		-probability that x takes a specific value = 0
		-also known as density function
		-standard normal: normal distribution with a mean of 0 and a standard deviation of 1
			-z-score: Z = (X-mu)/sigma = (X-mean)/standard deviation
		-normal distribution properties:
			-continuous, symmetric around the mean, bell-shaped, mean = median = mode, approaches horizontal axis on both sides of mean, 99.73% lie within three standard deviations of mean
				fully determined by mean and variance or standard deviation, location distribution on the number line depends on the mena of the distribution, single sigmas from mean = 68%,
				two sigmas from mean = 95%
	-combining independent random variables:
		-specific cases of random variables X, Y, means mu sub x and mu sub y, and variances (sigma sub x)**2 and (sigma sub y)**2
			Random Variable			Mean							Variance
			aX						a*(mu sub x)					(a**2)*((sigma sub x)**2)
			X + b					(mu sub x) + b					(sigma sub x)**2
			aX + b					a*(mu sub x) + b				(a**2)*((sigma sub x)**2)
			X + Y					(mu sub x) + (mu sub y)			(sigma sub x)**2  +   (sigma sub y)**2
			X - Y					(mu sub x) - (mu sub y)			(sigma sub x)**2  -   (sigma sub y)**2
			aX + bY					(a*(mu sub x)) - (a*(mu sub y)	((a**2)*(sigma sub x)**2) + ((b**2)*(sigma sub y)**2)
	-sampling distributions:
		-parameter: a numerical measure of a population -- example is a student's GPA
		-statistic: numerical measure of a sample -- example is the percentage of votes recieved by a President (not everyone votes, if so, it would be a parameter)
		-sampling distribution: the probability distribution of  statistic -- different samples of the same size from the same population will result in different statistic values
		-standard error: standard deviation of a statistic
		-central limit theorem: with a large enough number of population samples the distribution will be approximately normal 
			-mu sub x = mu and standard deviation sigma sub x  = sigma / sqrt(n)
			-as sample size n increases:
				-shape of distribution becomes more symmetric and bell shaped
				-center of distribution remains at mu
				-spread decreases and distribution becomes more peaked
		-sampling distribution of a sample proportion -- mu sub p = p and standard deviation sigma p = sqrt((p*(1-p))/n)
		-sampling distribution of a sample mean -- mu sub x = mu and standard deviation sigma sub x = sigma/sqrt(n)
		-sampling distribution of a difference between two independent sample proportions -- sigma sub(p1-p2) = sqrt(((p1*(1-p1))/n1)+((p2*(1-p2))/n2))
		
Statistical Inference: estimate unknown population characteristics, make inferences about unknown population characteristics
	-parameter: characteristic of a population
	-statistics: number computed from the sample
	-estimation: specific guess or value computed from a sample 
		-estimation process: procedure of guessing an unknown parameter value using hte observed values from samples
			-two types: point and interval
				-point estimation: gives a single value as an estimate of the unknown parameter and makes no allowance for uncertainty
				-interval estimation: recognizes uncertainty and specifies range of uncertainty
					-margin of error
	-point estimation continued
		-if A is the unknown population mean then the sample estimate is X so X is the estimate of mu (population mean)
			X = (X1 + X2 + ... + XN) / N
		-if A is the unknown population proportion P, then the point estimate is P:
			P = number of favorable occurrences / sample size
		-if A is the unknown popluation variance sigma**2 the the point estimate is S**2
			S**2 = sum(1 to N)(X sub N - X bar)**2 / (n-1)
	-sampling distribution: distribution of estimates from all possible samples of the same size from the same population
	-properties of a statistic
		-unbiasedness -- statistic is expected to give values centered on the unknown parameter value
			-bias(p) = E(p) - p where E(p) it the mean of sampling distribution (X bar)
		-variability -- degree of variation in a statistic's values 
			-statistics with small variability are said to be more effective
		-bias indicates that a guess is systematically wrong
		-inefficiency indicates that our guess is unsystematically wrong
		-confidence interval: goodness of estimate that gives two numbers within which the statistic is reasonably likely to be between
	-testing a hypothesis: decision making process about the value of a population parameter using information collected from a random sample
		-statistical hypothesis: claim or statement about a parameter value with two types (null and alternative)
			-null: statment that is assured to be true until proven otherwise
			-alternative hypothesis: statement about the aprameter that must be true if the null hypothesis is false
		-goal is to determine whether sample provides enough evidence for us to reject the null hypothesis
		-test statistic: computed from sampled data and used in process of testing hypothesis -- example in a normal model is the z-score
		-p-value: probability that we would observe a test statistic value at least as extreme as one computed from the sample if the null hypothesis were true 
			-large -- result is common if null hypothesis is true
			-small -- resutl would be rare if null hypothesis were true
		-one of the following decisions are made:
			-rejection of null hypothesis if p-value is less than alpha
			-because of lack of evidence we don't reject if p-value is greater than alpha (alpha is the probability of a type one error)
	-possible errors: (both decreased by increasing population size)
		-type I error: error of rejecting the null hypothesis when it is true, denoted by alpha
		-type II error: error of failing to reject the null hypothesis when it is false denoted by beta 
	-power of a test: probability that the null hypothesis will be rejected when a particular alternative value of the parameter is true -- a good test has high power
	-rejection region: set of test statistic values for which we should reject the null hypothesis, also called the critical region 
	-non-rejection region: set of test statistic values for which the null hypothesis should not be rejected 
		-critical value: boundary between rejection and non-rejection region, denoted z* or t*
		-left-tailed test: small values lead to rejection of null hypothesis
		-right-tailed test: large values lead to rejection of null hypothesis
		-two-tailed test: large or small values lead to rejection of null hypothesis
		-determining critical value example: investigator is willing to take 5% risk
			right-tailed -- above 95% or 0.95
			left-tailed -- below 5% or 0.5
			two-tailed -- outside of .025 and .975
