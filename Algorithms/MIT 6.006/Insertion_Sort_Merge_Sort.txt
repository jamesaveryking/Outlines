Insert Sort, Merge Sort 

Why sorting?
-obvious applications
-reduce cost --> complexity of problem solving
-Example -- determine middle of array
	array A[0:n] --> B[0:n] --> B[n/2] is the medium
	unsorted		 sorted
	-requires constant time with a sorted list

-Binary Search
	array A[0:n] --> B[0:n]
	unsorted		 sorted
	-O(log(n)) time cost when looking specific item k in n items
	-algorithm -- B[n/2]

-typically maintaining two objects sorted in both increasing and decreasing order
-sorting is an important subroutine of any application
-cocktail, bitonic
-specific algorithm suited to specific problem


Insertion Sort -- 5 lines of code -- theta (n**2)
	-advantage over merge -- merge sort requires auxiliary space (theta(n)) opposed to in-place sort (theta(n))
	
	-for i = [1:n] -- input to be sorted
	-insert A[i] into sorted array A[0:i-1] -- about to be increment with A[i]
		-by pair-wise swaps down to the correct position for the number in A[i]
	-Ex) 5 2 4 6 1 3 -- sort increasing L to R
		1) start at second element (key becomes 2 -- key = pointer to iteration through array using algorithm)
		2) swap 2 and 5 -- 2 5 4 6 1 3
		3) key = element 3, swap -- 2 4 5 6 1 3
		4) key = element 4, swap -- 2 4 5 6 1 3
		5) key = element 5, swap -- 2 4 5 1 6 3 -- shows fault in decreasing complexity
			5a:5d) swap 1 back to element 1 -- 1 2 4 5 6 3
		6) key = last element, swap -- 1 2 4 5 3 6
			6a:6c) swap 3 back to element 3 -- 1 2 3 4 5 6
		Total steps: theta(n) steps in terms of key positions
			-worst case -- theta(n**2 (-1 if starting key at 2?)) - at any given step you could have to perform n swaps if list was originally in:
				-decreasing sorting to increasing
				-increasing sorting to decreasing
	-Hypothetical: compares cost more than swaps
		-theta(n**2) comparison cost
		-need to change to decrease complexity
		-change pair-wise swaps to search to binary
			-bin_search(A[0:i-1]) -- makes theta(log(i))
			-changes worst case complexity to theta(n*log(n)) for compares
			-still worst case complexity of theta(n**2) for swaps
			
Merge Sort -- divide and conquer algorithm (like binary)
-theta(n*log(n))
-standard recursive algorithm
-array A split into two parts -- L & R
-merge pair of elements at leaves, merge 4 elements at one level up tree, 8 elements 2 levels up, etc.
-input size = n
-two arrays of size n/2 that are sorted -- true input
-output sorted size n
-EX) 8 elements
	-assumes two sorted arrays as input (L' and R')
	-L = 20 13 7 2
	-R = 12 11 9 1
	-two "fingers" that point to elements in each array and compare across arrays to sorted array
	1) compare 2 and 1 --two-finger algorithm = 1
	2) compare 2 and 7 -- 1 2
	3) compare 7 and 9 -- 1 2 7
	4) compare 9 and 13 -- 1 2 7 9
	5) compare 13 and 11 -- 1 2 7 9 11
	6) compare 13 and 12 -- 1 2 7 9 11 12
	7) compare 13 and 20 -- 1 2 7 9 11 12 13
	8) 1 2 7 8 11 12 13 20
	-complexity -- theta(n)
	-overall complexity -- theta(n*log(n))
	-complexity = T(n) = constant time to divide array + 2 sides * recursive call of (n/2) + constant * n (merge portion)
							C1 + 2T(n/2) + c*n
	-Tree Picture of Merge:
					c*n						sum = c*n ]
			c*(n/2)			c*(n/2)			sum = c*n ]	T(n) = (1+log(n))*(c*n) = n*log(n)
	c*(n/4)		c*(n/4) c*(n/4)		c*(n/4)	sum = c*n ]
	
	- leaves = n (width)
	- tree branches = 1+log(n) (height)


Recurrence Solving
